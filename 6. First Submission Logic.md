# Logic For First Submission

## Task 1: Write a job to consume clickstream data from Kafka and ingest to Hadoop

### Step 1. Create a python file (vi spark_kafka_to_local.py) using vi command. This file will contain the code which will ingest the relevant data from kafka in Hadoop.

- export SPARK_KAFKA_VERSION=0.10
- wget https://ds-spark-sql-kafka-jar.s3.amazonaws.com/spark-sql-kafka-0-10_2.11-2.3.0.jar
- vi spark_kafka_to_local.py
  ![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/2f577d87-7325-490f-bacc-6230ecefe759)

### Step 2. Explaining the code in spark_kafka_to_local.py 

#### A.	Importing the modules.
import sys 
import os 
from pyspark.sql import SparkSession  
from pyspark.sql.functions import *  
from pyspark.sql.types import *   
from pyspark.sql.functions import from_json  
from pyspark.sql.window import Window  

#### B.	Initializing the Spark Session
spark = SparkSession \  
 .builder \   
 .master("local") \   
 .appName("CapstoneProject") \   
 .getOrCreate()   
spark.sparkContext.setLogLevel('ERROR')  

#### C.	Creating a dataframe Kafka Data


clickstream_df = spark \  
    .readStream \  
    .format("kafka") \  
    .option("kafka.bootstrap.servers", "18.211.252.152:9092") \  
    .option("subscribe", "de-capstone5") \  
    .option("startingOffsets", "earliest") \  
    .load()
#### D.	Print the schema of the dataframe

clickstream_df.printSchema()  

#### E.	Transform the Dataframe by dropping a few columns and changing the value column datatype.

clickstream_df = clickstream_df \  
           .withColumn('value_str', clickstream_df['value'].cast('string').alias('key_str')).drop('value') \  
           .drop('key','topic','partition','offset','timestamp','timestampType')  

#### F.	Write the dataframe to the local file directory and keep it running until terminated.  

clickstream_df.writeStream \  
    .outputMode("append") \  
    .format("json") \  
    .option("truncate","false") \  
    .option("path", "clickstream_data") \  
    .option("checkpointLocation","clickstream_checkpoint") \  
    .start() \  
    .awaitTermination()  

### Step 3. Run the Spark Submit Command to ingest the relevant data from Kafka to Hadoop.  
export SPARK_KAFKA_VERSION=0.10  
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 spark_kafka_to_local.py  
![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/fe725ccc-e2bf-4eb4-9e1a-27162d33c787)  

![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/dcdb0370-4de6-4222-9bb9-8e47d129ccbb)  

### Step 4. Create another python file (spark_local_flatten.py) using vi command. This file will clean the loaded Kafka data to a more structured format.  

vi spark_local_flatten.py  

### Step 5. Explaining the code in spark_local_flatten.py  

#### A.	Importing the modules  

from pyspark.sql import SparkSession  
from pyspark.sql.functions import *  

#### B.	Initializing Spark Session  

spark = SparkSession.builder \  
    .master("local") \  
    .appName("CapstoneProject") \  
    .getOrCreate()  

#### C.	Reading json data into dataframe  

clickstream_df = spark.read.json('clickstream_data/*')  

#### D.	Extrating columns from json value in dataframe and create new dataframe with new cloumns  

clickstream_df = clickstream_df.select(\  
	get_json_object(clickstream_df["value_str"],"$.customer_id").alias("customer_id"),\  
	get_json_object(clickstream_df["value_str"],"$.app_version").alias("app_version"),\  
	get_json_object(clickstream_df["value_str"],"$.OS_version").alias("OS_version"),\  
	get_json_object(clickstream_df["value_str"],"$.lat").alias("lat"),\  
	get_json_object(clickstream_df["value_str"],"$.lon").alias("lon"),\  
	get_json_object(clickstream_df["value_str"],"$.page_id").alias("page_id"),\  
	get_json_object(clickstream_df["value_str"],"$.button_id").alias("button_id"),\  
	get_json_object(clickstream_df["value_str"],"$.is_button_click").alias("is_button_click"),\  
	get_json_object(clickstream_df["value_str"],"$.is_page_view").alias("is_page_view"),\  
	get_json_object(clickstream_df["value_str"],"$.is_scroll_up").alias("is_scroll_up"),\  
	get_json_object(clickstream_df["value_str"],"$.is_scroll_down").alias("is_scroll_down"),\  
	get_json_object(clickstream_df["value_str"],"$.timestamp\n").alias("timestamp"),\  
	)  

#### E.	Printing the Dataframe schema  

print(clickstream_df.schema)  

#### F.	Printing 10 records from the dataframe  

clickstream_df.show(10)  

![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/82263fa3-0efa-4adf-8b15-bbde10d2e4e2)  

#### G.	Saving the dataframe to csv file with headers in local file directory  

clickstream_df.coalesce(1).write.format('csv').mode('overwrite').save('clickstream_data_flatten', header = 'true')  

### Step 6. Run the spark submit command  

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 spark_local_flatten.py  

![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/9c231572-53e3-49f2-9c62-ac65009e5a87)  






## Task 2: Write a script to ingest the relevant bookings data from AWS RDS to Hadoop  

### Step 1. First we need to setup MySQL Connector on AWS EMR  

#### A.	Run the following command to install the MySQL connector jar file:  

- a.	wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz  
  ![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/820b5114-ac0c-47bb-91aa-ffa0e67e678d)  

#### B.	Run the following step to extract the MySQL connector tar file:  

- a.	tar -xvf mysql-connector-java-8.0.25.tar.gz  
  ![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/afe493e8-7f0f-4b2c-8a2b-19f11c450490)  

#### C.	Go to the MySQL Connector directory and then copy it to the Sqoop library to complete the installation.  

- a.	cd mysql-connector-java-8.0.25/  
- b.	sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/  
  ![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/dcc7787e-6db5-4eb7-b472-4b41e81bd1c3)  

### Step 2: Next, run the Sqoop import command to import data from AWS RDS.  

#### A.	Use sqoop import command to import the data:  

sqoop import \ 
--connect jdbc:mysql://upgraddetest.cyaielc9bmnf.us-east-1.rds.amazonaws.com/testdatabase \  
--table bookings \ 
--username student --password STUDENT123 \  
--null-string '\\N' --null-non-string '\\N'  \  
--target-dir bookings_data \  
-m 1  

![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/db66a77b-20cf-4c64-842f-c8e3be3e8585)  

![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/2749e2c9-39b4-435a-9748-871f4c85cfb7)  

#### B.	Use this command to check the imported data  

- a.	hadoop fs -ls bookings_data  
  ![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/02eae1a0-c55c-4dbf-b1e9-96225c5bbfbd)  

- b.	hadoop fs -cat bookings_data/part-m-00000 | wc -l  
  ![image](https://github.com/Abhijeet-Sih/Data-Capture-and-Analysis-of-Cab-Rides/assets/77975708/d22523bc-cee7-44c4-9e2b-72610a78c0e0)  






